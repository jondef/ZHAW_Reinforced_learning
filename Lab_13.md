# Week 13: Graded Lab - DQN lands on the Moon

## Lab session

Welcome to the second and last graded lab of our Reinforcement Learning course! This week, we are delving into the integration of deep learning with the REINFORCE algorithm. In this lab, you'll be tasked with implementing the Deep Q-Network (DQN) algorithm for the Lunar Lander Gym environment. This lab aims to test your understanding and implementation skills with a more complex problem. Please note that this notebook requires a GPU to run efficiently, and Google Colab is recommended for ease of access.

## Educational Objectives

- Understand the fundamentals of the Deep Q-Network (DQN) algorithm and its application in reinforcement learning.
- Implement the DQN algorithm in a complex environment like Lunar Lander, demonstrating your coding and problem-solving skills.
- Gain practical experience in training and fine-tuning deep reinforcement learning models for more challenging environments.
- Evaluate the performance of your DQN implementation based on various metrics, such as average reward, convergence, and stability.

## Getting Started

Please group up in pairs and download and open the Jupyter notebook either on Google Colab or on your local machine. To begin, please access the notebook titled "DQN_lands_on_the_moon" provided for this lab session. Note that Google Colab is recommended for this exercise due to the requirement for GPU acceleration.

## Tasks

- **Implementation of DQN**: Implement the DQN algorithm in the provided notebook for the Lunar Lander environment. Note that we are assuming you to implement the network with PyTorch again, but you are free to implement it in TensorFlow if that suits you more.
- **Training and Tuning**: Train your DQN model on the Lunar Lander environment. Observe its performance and make necessary adjustments to the hyperparameters to improve its performance.
- **Performance Metrics**: Evaluate and report the performance of your DQN implementation based on metrics such as average episode reward, episodes required for convergence, and any additional relevant statistics.
- **Plotting Outputs and Creating a Video**: Create a video of the fully trained agent running one episode. To do this, you can take a look at the lab description of week 11, where we provided you a snippet to create a video for a given agent and environment. We expect you to show us a video of your final DQN agent.

## Submission

The deadline for completing this graded lab is the start of the lab session in week 13 (12th of December, 16:00). If you are attending the lecture in person, please be prepared to show your results upfront. If you are attending online, you should send an email to embe@zhaw.ch with your lab results attached. Make sure to include the names of both students if you are working in a pair.

## Key Takeaways

- **Complexity and Scalability**: Understand that as environments become more complex, algorithms like DQN offer scalable solutions for training agents in challenging scenarios.
