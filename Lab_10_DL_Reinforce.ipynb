{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbhF6-H5x_b7"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDr7ZGY-x_b9"
      },
      "source": [
        "# Q-Learning on the CartPole Environment\n",
        "\n",
        "This is an altered version of Jose Nieves Flores Maynez' notebook.\n",
        "\n",
        "This tutorial shows how to use Q-Learning to train an RL agent on the CartPole-v0 task from the [OpenAI Gym](https://gym.openai.com/).\n",
        "\n",
        "![cartpole](https://github.com/pytorch/tutorials/blob/main/_static/img/cartpole.gif?raw=true)\n",
        "\n",
        "The Cartpole environment is a common simple example that is used often for simple RL examples.\n",
        "\n",
        "In this environment, the task is to balance the pole that is attached to the cart, by moving the cart to either side.\n",
        "The reward gets incremented for each step (for up to 200 steps) where the pole is not exceeding a set angle and the cart is not touching the sides of the line.\n",
        "The environment provides four parameters that represent the state of the environment:\n",
        "Position and velocity of the cart and angle and angular velocity of the pole (see [the documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/#observation-space)).\n",
        "We will solve this by applying Q-Learning to our RL agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Packages\n",
        "\n",
        "\n",
        "First, let's import needed packages."
      ],
      "metadata": {
        "id": "QCWbuk66234H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ccdmXWUx_b_"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "# Hugging Face Hub\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if we have a GPU"
      ],
      "metadata": {
        "id": "XYndtRLbU11C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "hrJ2gg3LU1HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psxbGvWJx_cB"
      },
      "source": [
        "## Implementation\n",
        "Since this algorithm relies on updating a function for each existing pair of state and action, environments that have a high state-space become problematic. This is because we can approximate better the actual value of a state-action pair as we visit it more often. However, if we have many states or many actions to take, we distribute our visits among more pairs and it takes much longer to converge to the actual true values. The CartPole environment gives us the position of the cart, its velocity, the angle of the pole and the velocity at the tip of the pole as descriptors of the state. However, all of these are continuous variables. To be able to solve this problem, we need to discretize these states since otherwise, it would take forever to get values for each of the possible combinations of each state, despite them being bounded. The solution is to group several values of each of the variables into the same “bucket” and treat them as similar states. The agent implemented for this problem uses 3, 3, 6, and 6 buckets respectively."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "DLHhz65OabE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReinforceAgent(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        \"\"\"\n",
        "        Initialize agent.\n",
        "\n",
        "        :param s_size: The size of the state vector\n",
        "        :param a_size: The size of the action vector\n",
        "        :param h_size: The size of the hidden state vector\n",
        "        \"\"\"\n",
        "        super(ReinforceAgent, self).__init__()\n",
        "        # TODO: Create two fully connected layers\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define the forward pass\n",
        "        # state goes to fc1 then we apply ReLU activation function\n",
        "        x =\n",
        "\n",
        "        # fc1 outputs goes to fc2\n",
        "        x =\n",
        "\n",
        "        # We output the softmax\n",
        "        return\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Given a state, take action\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)\n",
        "\n",
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "    # Help us to calculate the score during the training\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "\n",
        "    for i_episode in trange(1, n_training_episodes + 1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "\n",
        "        returns = deque(maxlen=max_t)\n",
        "        n_steps = len(rewards)\n",
        "        # Compute the discounted returns at each timestep,\n",
        "        # as\n",
        "        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
        "        #\n",
        "        # In O(N) time, where N is the number of time steps\n",
        "        # (this definition of the discounted return G_t follows the definition of this quantity\n",
        "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
        "        # G_t = r_(t+1) + r_(t+2) + ...\n",
        "\n",
        "        # Given this formulation, the returns at each timestep t can be computed\n",
        "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
        "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
        "        # G_(t-1) = r_t + gamma* G_t\n",
        "        # (this follows a dynamic programming approach, with which we memorize solutions in order\n",
        "        # to avoid computing them multiple times)\n",
        "\n",
        "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
        "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
        "\n",
        "        ## Given the above, we calculate the returns at timestep t as:\n",
        "        #               gamma[t] * return[t] + reward[t]\n",
        "        #\n",
        "        ## We compute this starting from the last timestep to the first, in order\n",
        "        ## to employ the formula presented above and avoid redundant computations that would be needed\n",
        "        ## if we were to do it from first to last.\n",
        "\n",
        "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
        "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
        "        ## a normal python list would instead require O(N) to do this.\n",
        "        for t in range(n_steps)[::-1]:\n",
        "            disc_return_t = returns[0] if len(returns) > 0 else 0\n",
        "            returns.appendleft(gamma * disc_return_t + rewards[t])\n",
        "\n",
        "        ## standardization of the returns is employed to make training more stable\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "        ## eps is the smallest representable float, which is\n",
        "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "        policy_loss = []\n",
        "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * disc_return)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i_episode % print_every == 0:\n",
        "            tqdm.write(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 1000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"state_space\": env.observation_space.shape[0],\n",
        "    \"action_space\": env.action_space.n,\n",
        "}\n",
        "\n",
        "\n",
        "def load_reinforce_agent():\n",
        "    cartpole_policy = ReinforceAgent(\n",
        "        cartpole_hyperparameters[\"state_space\"],\n",
        "        cartpole_hyperparameters[\"action_space\"],\n",
        "        cartpole_hyperparameters[\"h_size\"],\n",
        "    ).to(device)\n",
        "    cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])\n",
        "    scores = reinforce(\n",
        "        cartpole_policy,\n",
        "        cartpole_optimizer,\n",
        "        cartpole_hyperparameters[\"n_training_episodes\"],\n",
        "        cartpole_hyperparameters[\"max_t\"],\n",
        "        cartpole_hyperparameters[\"gamma\"],\n",
        "        100,\n",
        "    )\n",
        "    return cartpole_policy\n",
        "\n",
        "agent = load_reinforce_agent()"
      ],
      "metadata": {
        "id": "tqqVqJYZwjQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "    \"\"\"\n",
        "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "    :param env: The evaluation environment\n",
        "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "    :param policy: The Reinforce agent\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state = env.reset()\n",
        "        step = 0\n",
        "        done = False\n",
        "        total_rewards_ep = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action, _ = policy.act(state)\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            total_rewards_ep += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            state = new_state\n",
        "        episode_rewards.append(total_rewards_ep)\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "\n",
        "    return mean_reward, std_reward\n",
        "\n",
        "evaluate_agent(\n",
        "    env, cartpole_hyperparameters[\"max_t\"], cartpole_hyperparameters[\"n_evaluation_episodes\"], agent\n",
        ")"
      ],
      "metadata": {
        "id": "ssL5Q92eXEko"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}